{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_GA_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuVqdxiNDiso"
      },
      "source": [
        "Reusing previous Assignment's IELM code to perform classification on MNIST data set. \n",
        "\n",
        "Also, a genetic algorithm is used as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biHlFkWLYDE-"
      },
      "source": [
        "# importing necessary libraries to run the model\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from functools import partial"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sToMhzNwWC3-"
      },
      "source": [
        "# defining necessary functions \n",
        "\n",
        "# defining function to get the optimized value\n",
        "def _best(populace, funcFitness, best, bestFitness):\n",
        "\n",
        "    # best = None, bestFitness = None\n",
        "    for i in range(populace[0].shape[0]):\n",
        "        \n",
        "        if best is None or valueTmp < bestFitness:\n",
        "            bestFitness = valueTmp\n",
        "            best = populace[0][i][np.newaxis, :]\n",
        "            \n",
        "        if populace[1][i] > -1.0:\n",
        "            valueTmp = populace[1][i]\n",
        "        else:\n",
        "            valueTmp = funcFitness(populace[0][i][np.newaxis, :])\n",
        "            populace[1][i] = valueTmp\n",
        "\n",
        "    return best, bestFitness\n",
        "\n",
        "# defining a function to provide individual score\n",
        "def _candidate(size):\n",
        "    return np.random.randint(2, size=size)\n",
        "\n",
        "# defining a function to provide score for collection of individuals\n",
        "def _select_family(populace, funcFitness):\n",
        "\n",
        "    pop_id = np.random.permutation(np.arange(populace[0].shape[0]))\n",
        "\n",
        "    parent1 = populace[0][pop_id[0], :][np.newaxis, :]\n",
        "    parent2 = populace[0][pop_id[1], :][np.newaxis, :]\n",
        "\n",
        "    # condition for parent fitness in the populace\n",
        "    if populace[1][pop_id[0]] < -1.0:\n",
        "        parent1Fitness = funcFitness(parent1)\n",
        "        populace[1][pop_id[0]] = parent1Fitness\n",
        "    else:\n",
        "        parent1Fitness = populace[1][pop_id[0]]\n",
        "\n",
        "    if populace[1][pop_id[1]] < -1.0:\n",
        "        parent2Fitness = funcFitness(parent2)\n",
        "        populace[1][pop_id[1]] = parent2Fitness\n",
        "    else:\n",
        "        parent2Fitness = populace[1][pop_id[1]]\n",
        "\n",
        "    return parent1 if parent1Fitness < parent2Fitness else parent2\n",
        "\n",
        "# defining the genetic algorithm that will provide the optimized score\n",
        "def genetic_algorithm(funcFitness, dim, n_candidate=10, epochs=50, hybrid_rate=0.9, mutation_rate=0.1):\n",
        "    \n",
        "    assert n_candidate % 2 == 0\n",
        "    \n",
        "    populace = [np.array([_candidate(dim) for _ in range(n_candidate)]),\n",
        "                  np.zeros(n_candidate) - 1.0]\n",
        "\n",
        "    children = np.zeros((n_candidate, dim))\n",
        "\n",
        "    best, bestFitness = None, None\n",
        "\n",
        "    return best, bestFitness\n",
        "\n",
        "# defining a Function that evolves a candidate\n",
        "def _evolve(candidate):\n",
        "\n",
        "    candidate_id = np.random.randint(high=candidate.shape[0], low = 0)\n",
        "\n",
        "    # flipping a bit\n",
        "    candidate[candidate_id] = 1 - candidate[candidate_id] \n",
        "    return candidate\n",
        "\n",
        "def _hybrid(male, female):\n",
        "    candidate_id = np.random.randint(0, male.shape[1], 2)\n",
        "    candidate_id.sort()\n",
        "\n",
        "    child2 = np.hstack((female[0, :candidate_id[0]], male[0, candidate_id[0]:candidate_id[1]], female[0, candidate_id[1]:]))[np.newaxis, :]\n",
        "    child1 = np.hstack((male[0, :candidate_id[0]], female[0, candidate_id[0]:candidate_id[1]], male[0, candidate_id[1]:]))[np.newaxis, :]\n",
        "\n",
        "    return child1, child2   \n",
        "\n",
        "# defining a sigmoid function as the activation function\n",
        "def _sigmoid(x):\n",
        "    return 1. / (1. + np.exp(-x))\n",
        "\n",
        "# defining a fourier function as the activation function\n",
        "def _fourier(x):\n",
        "    return np.sin(x)\n",
        "\n",
        "# initialising a simple identity function for the value inputed\n",
        "def _identity(x):\n",
        "    return x\n",
        "\n",
        "# initialising a function to compute the activation of the model, sigmoid is used for now. Fourier and Hard limit can be used as well\n",
        "def activation_fn_compute(name):\n",
        "    return {\n",
        "        'sigmoid': _sigmoid,\n",
        "        'fourier': _fourier,\n",
        "    }[name]\n",
        "\n",
        "# a function to compute mean square error\n",
        "def _mean_squared_error(y, pred):\n",
        "    return 0.5 * np.mean((y - pred) ** 2)\n",
        "\n",
        "# intialising a function to compute the loss of the model, mse is used for now\n",
        "def loss_compute(name):\n",
        "    return {\n",
        "        'mse': _mean_squared_error,\n",
        "    }[name]\n",
        "\n",
        "# compute number of neurons\n",
        "def compute_number_nodes(input_nodes, output_nodes):\n",
        "    return int(np.ceil(np.sqrt(input_nodes * output_nodes)))\n",
        "\n",
        "def _wrapper_FitnessFunc(funcFitness, _input_nodes, _output_nodes, X, Y, mask):\n",
        "\n",
        "  X_Train = mask_input(X, mask)\n",
        "  omega = 0.1\n",
        "  train_score = funcFitness(input_nodes, output_nodes, X_train, Y_train)\n",
        "  score = rho * (1.0 - train_score) + omega * mask.sum().astype(np.float) / mask.shape[1]\n",
        "  return score\n",
        "\n",
        "# Incremental Extreme machine learning class is initialised so that model can be run\n",
        "class IELM:\n",
        "  \n",
        "  # defining a function to initialise all the necessary variables\n",
        "  def __init__(self, input_nodes, hidden_nodes, output_nodes, activation='sigmoid',\n",
        "                loss='mse', beta_init=None, w_init=None, bias_init=None):\n",
        "    # declaring the values for the input, hidden and output nodes for the model\n",
        "    # w is the weight vector\n",
        "      self._input_nodes = input_nodes\n",
        "      self._hidden_nodes = 1\n",
        "      self._output_nodes = output_nodes\n",
        "\n",
        "    # declaring activation and loss for the model\n",
        "      self._activation = activation_fn_compute(activation)\n",
        "      self._loss = loss_compute(loss)\n",
        "\n",
        "    # computing alpha value from input and hidden nodes\n",
        "      self._w = np.random.uniform(-1, 1, size=(self._input_nodes, self._hidden_nodes))\n",
        "    \n",
        "    # computing bias value from hidden nodes\n",
        "      self._bias = np.zeros(shape=(self._hidden_nodes,))\n",
        "\n",
        "    # computing beta value from hidden and output nodes\n",
        "      self._beta = np.random.uniform(-1., 1., size=(self._hidden_nodes, self._output_nodes))\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "      return 1. / (1. + np.exp(-x))\n",
        "\n",
        "  # function to place them in lists\n",
        "  def predict(self, X):\n",
        "      return list(self(X))\n",
        "\n",
        "  def genetic_algorthm(self, X, Y, mask, best):\n",
        "    pred = self.predict(X)\n",
        "    # compute Loss\n",
        "    loss = self._loss(Y, pred)\n",
        "\n",
        "    # compute Accuracy\n",
        "    acc = np.sum(np.argmax(pred, axis=-1) == np.argmax(Y, axis=-1)) / len(Y)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "  # a function to run the exact model which takes parameters \n",
        "  def fit(self, X, Y, LMax, display_time=False):\n",
        "      \n",
        "      # computing the value with sigmoid activation function \n",
        "      # H is the hidden layer output matrix\n",
        "      H = self.sigmoid(X.dot(self._w))\n",
        "\n",
        "      # pseudoinverse of the hidden layer matrix\n",
        "      H_pinv = np.linalg.pinv(H)\n",
        "\n",
        "      # beta gets updated\n",
        "      self._beta = H_pinv.dot(Y)\n",
        "\n",
        "      # looping it within the range of LMax which will be initialised later\n",
        "      for i in range(2,LMax):\n",
        "        # initialising random for beta and w(alpha)\n",
        "          beta_random = np.random.uniform(-1.,1.,size=(1, self._output_nodes))\n",
        "          alpha_random = np.random.uniform(-1.,1.,size=(self._input_nodes, 1))\n",
        "          self._w=np.hstack([self._w,alpha_random])\n",
        "\n",
        "          # update the shape everytime the loop runs\n",
        "          \n",
        "          self._beta = np.vstack([self._beta,beta_random])\n",
        "          H = self.sigmoid(X.dot(self._w))\n",
        "          \n",
        "          # pseudoinverse \n",
        "          H_pinv = np.linalg.pinv(H)\n",
        "          \n",
        "          # below step updates beta\n",
        "          self._beta = H_pinv.dot(Y)\n",
        "\n",
        "      # to provide the time taken for training \n",
        "      if display_time:\n",
        "          start = time.time()\n",
        "      H_pinv = np.linalg.pinv(H)\n",
        "      if display_time:\n",
        "          stop = time.time()\n",
        "          print(f'Train time: {stop-start}')\n",
        "\n",
        "      self._beta = H_pinv.dot(Y)\n",
        "\n",
        "  def __call__(self, X):\n",
        "      H = self._activation(X.dot(self._w) + self._bias)\n",
        "      return H.dot(self._beta)\n",
        "\n",
        "  # initialising a function to evaluate the loss and accuracy of the model\n",
        "  def compute_loss_acc(self, X, Y):\n",
        "      pred = self.predict(X)\n",
        "\n",
        "      # compute Loss\n",
        "      loss = self._loss(Y, pred)\n",
        "\n",
        "      # compute Accuracy\n",
        "      acc = np.sum(np.argmax(pred, axis=-1) == np.argmax(Y, axis=-1)) / len(Y)\n",
        "\n",
        "      return loss, acc"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AvpbhklWZx5"
      },
      "source": [
        "# Initialising model variables and declaring them\n",
        "classes = 10\n",
        "hidden_layers = 512\n",
        "\n",
        "# input lenght is 28**2 as it is 28x28 \n",
        "input_length = 28**2\n",
        "\n",
        "LMax=100"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqqKC1iVWcXV"
      },
      "source": [
        "# Loading MNIST Dataset using the library functionality of Keras\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Processing each image which is a 28*28 pixel into input vectors between 0 and 255 for training\n",
        "x_train = x_train.astype(np.float32) / 255.\n",
        "x_train = x_train.reshape(-1, input_length)\n",
        "\n",
        "# Processing each image which is a 28*28 pixel into input vectors between 0 and 255 for testing\n",
        "x_test = x_test.astype(np.float32) / 255.\n",
        "x_test = x_test.reshape(-1, input_length)\n",
        "\n",
        "# converting the value to categorical\n",
        "y_train = to_categorical(y_train, classes).astype(np.float32)\n",
        "y_test = to_categorical(y_test, classes).astype(np.float32)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bbsmNTmWezq"
      },
      "source": [
        "# create instance of our model\n",
        "model = IELM(\n",
        "    input_length,\n",
        "    hidden_layers,\n",
        "    classes\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZXV3XTUWhcX",
        "outputId": "6659c87e-0b43-4bc5-8481-27dbd91f0257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train model and compute accuracy and loss\n",
        "model.fit(x_train, y_train, LMax, display_time=True)\n",
        "\n",
        "# computing time taken\n",
        "timeTaken_train = time.time()\n",
        "\n",
        "# loss and accuracy from the GA optimizer\n",
        "optimizedGA_loss, accuracy_train = model.genetic_algorthm(x_train, y_train, mask=None, best=None)\n",
        "final_timeTaken_train = time.time()\n",
        "print('Training Loss:', optimizedGA_loss)\n",
        "print('GAs best accuracy:', accuracy_train)\n",
        "print('Total Time require for Training is (in Seconds):', (final_timeTaken_train-timeTaken_train))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train time: 0.9560821056365967\n",
            "Training Loss: 0.023392366898266015\n",
            "GAs best accuracy: 0.8033166666666667\n",
            "Total Time require for Training is (in Seconds): 0.8445286750793457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXYTuD6YW3lI",
        "outputId": "ee59f10b-55df-4504-cc23-4f424615319d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#  Test model and compute accuracy and loss\n",
        "timeTaken_test = time.time()\n",
        "\n",
        "# loss and accuracy from the GA optimizer\n",
        "optimizedGA_loss_test, accuracy_test = model.genetic_algorthm(x_test, y_test, mask=None, best=None)\n",
        "final_timeTaken_test = time.time()\n",
        "print('GAs optimized Testing loss:', optimizedGA_loss_test)\n",
        "print('Testing accuracy:', accuracy_test)\n",
        "print('Total Time required for Testing is (in seconds):', (final_timeTaken_test-timeTaken_test))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GAs optimized Testing loss: 0.023046650914068156\n",
            "Testing accuracy: 0.8103\n",
            "Total Time required for Testing is (in seconds): 0.1635141372680664\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}